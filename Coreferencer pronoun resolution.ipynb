{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP Project.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOxM7yhMGCKJ",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/google-research-datasets/gap-coreference           \n",
        "https://www.kaggle.com/c/gendered-pronoun-resolution/data           \n",
        "https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asSFJPZ4GCKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtvnDZw8GCKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, string, nltk\n",
        "import zipfile\n",
        "import sys, os, time\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_auc_score,log_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H52mMARDGCKW",
        "colab_type": "code",
        "outputId": "4731c837-a9c7-40dc-a250-f2dcadbc8330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input,Dense,Dropout,BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsCO1hFZGCKa",
        "colab_type": "code",
        "outputId": "c7cabd1b-34a0-43c4-c4c5-51f673b895c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4OC3InMGCKd",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4YKgjLeGCKe",
        "colab_type": "code",
        "outputId": "d5648bae-b474-41d2-b9d8-417e1b0b170f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "train = pd.read_csv('https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv', sep='\\t')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv', sep='\\t')\n",
        "val = pd.read_csv('https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv', sep='\\t')\n",
        "df = pd.concat([train,val,test]).reset_index(drop=True)\n",
        "df.shape[0], train.shape[0], val.shape[0], test.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7eebcc99bb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcVUx9vlGCKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8pVGR8XGCKr",
        "colab_type": "text"
      },
      "source": [
        "## Baseline on preceding and succeeding 5 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "401JGymVGCKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(text, offset, l):\n",
        "    f = []\n",
        "\n",
        "    if offset > 5:    \n",
        "        f = text[offset-5:offset]\n",
        "    else:\n",
        "        x = ['<unk>']*5\n",
        "        x[5-offset:5] = text[:offset]\n",
        "        f = x        \n",
        "        \n",
        "    if (len(text)-offset+l) > 5:\n",
        "        f += text[offset:offset+5]\n",
        "    else:\n",
        "        x = ['<unk>']*5\n",
        "        x[-5:-5+offset] = text[offset:]\n",
        "        f += x\n",
        "        \n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6euKHEFGCKu",
        "colab_type": "text"
      },
      "source": [
        "#### Extract features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DN8pkVBYGCKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = []\n",
        "\n",
        "for i,row in enumerate(df.values):\n",
        "    text = row[1]    \n",
        "    a_offset = len(text[:row[5]].split())\n",
        "    a_len = len(row[4].split())\n",
        "    \n",
        "    b_offset = len(text[:row[8]].split())\n",
        "    b_len = len(row[7].split())\n",
        "    \n",
        "    text = text.split()\n",
        "    f = get_features(text, a_offset, a_len)\n",
        "    f += get_features(text, b_offset, b_len)\n",
        "    \n",
        "    features += [f]\n",
        "\n",
        "features = pd.DataFrame(features).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuuLrbwJGCKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = ['<unk>']\n",
        "for row in df['Text']:\n",
        "    y += row.split()\n",
        "le = preprocessing.LabelEncoder().fit(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eHoL38aGCKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = le.transform(features.flatten().astype(str))\n",
        "features = features.reshape((df.shape[0],20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7j8Ovr_lGCLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "for i,row in enumerate(df[['A-coref','B-coref']].values):\n",
        "    if row[0] == True:\n",
        "        labels += [0]\n",
        "    elif row[1] == True:\n",
        "        labels += [1]\n",
        "    else:\n",
        "        labels += [2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuJxBZ8-GCLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = features[:3000].copy()\n",
        "X_test = features[3000:].copy()\n",
        "Y_train = labels[:3000].copy()\n",
        "Y_test = labels[3000:].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Z-wWtYGCLQ",
        "colab_type": "text"
      },
      "source": [
        "#### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5iD2CB2GCLR",
        "colab_type": "code",
        "outputId": "35cd1a66-a41b-45c0-b40d-c1ba4ab11170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
        "pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL2M7e28GCLd",
        "colab_type": "code",
        "outputId": "e92ab270-f2ed-434d-9bab-7c22f1235df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "precision = precision_score(pred,Y_test,average='weighted')\n",
        "recall = recall_score(pred,Y_test,average='weighted')\n",
        "f1 = f1_score(pred,Y_test,average='weighted')\n",
        "accuracy = accuracy_score(pred, Y_test)\n",
        "\n",
        "print('precision:',precision,\n",
        "  '\\nrecall:',recall,'\\nf1:',f1,'\\nacc:',accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: 0.6872397451128324 \n",
            "recall: 0.6031636863823934 \n",
            "f1: 0.6412982346833844 \n",
            "acc: 0.6031636863823934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RoHXr-8GCLq",
        "colab_type": "text"
      },
      "source": [
        "## Baseline on preceding and succeeding 5 POS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn64So2-GCLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(text, offset, l):\n",
        "    f = []\n",
        "\n",
        "    if offset > 5:    \n",
        "        f = text[offset-5:offset]\n",
        "    else:\n",
        "        x = ['<unk>']*5\n",
        "        x[5-offset:5] = text[:offset]\n",
        "        f = x        \n",
        "        \n",
        "    if (len(text)-offset+l) > 5:\n",
        "        f += text[offset:offset+5]\n",
        "    else:\n",
        "        x = ['<unk>']*5\n",
        "        x[-5:-5+offset] = text[offset:]\n",
        "        f += x\n",
        "        \n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kMb1S0EGCL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = []\n",
        "\n",
        "for i,row in enumerate(df.values):\n",
        "    text = row[1]    \n",
        "    a_offset = len(text[:row[5]].split())\n",
        "    a_len = len(row[4].split())\n",
        "    \n",
        "    b_offset = len(text[:row[8]].split())\n",
        "    b_len = len(row[7].split())\n",
        "    \n",
        "    tokens = np.array(nltk.pos_tag(text.split()))[:,1]\n",
        "    tokens = [token for token in tokens]\n",
        "    f = get_features(tokens, a_offset, a_len)\n",
        "    f += get_features(tokens, b_offset, b_len)\n",
        "    \n",
        "    features += [f]\n",
        "    \n",
        "features = pd.DataFrame(features).fillna('<unk>').values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8mhEutOGCL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = [('<unk>','<unk>')]\n",
        "for row in df['Text']:\n",
        "    y += nltk.pos_tag(row.split())\n",
        "y = np.array(y)[:,1]\n",
        "le = preprocessing.LabelEncoder().fit(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na6QsiL6GCL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = le.transform(features.flatten().astype(str))\n",
        "features = features.reshape((df.shape[0],20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otqd_zgiGCL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = features[:3000].copy()\n",
        "X_test = features[3000:].copy()\n",
        "Y_train = labels[:3000].copy()\n",
        "Y_test = labels[3000:].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szOBjnryGCL_",
        "colab_type": "text"
      },
      "source": [
        "#### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEasakp7GCMA",
        "colab_type": "code",
        "outputId": "eff149dd-986a-47a4-df36-f13ea6067b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
        "pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqksWPSrGCMB",
        "colab_type": "code",
        "outputId": "22709ffe-0e21-41fb-b1f8-8c8c34fd47ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "precision = precision_score(pred,Y_test,average='weighted')\n",
        "recall = recall_score(pred,Y_test,average='weighted')\n",
        "f1 = f1_score(pred,Y_test,average='weighted')\n",
        "accuracy = accuracy_score(pred, Y_test)\n",
        "\n",
        "print('precision:',precision,\n",
        "  '\\nrecall:',recall,'\\nf1:',f1,'\\nacc:',accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: 0.675903821932107 \n",
            "recall: 0.6052269601100413 \n",
            "f1: 0.6371361934578361 \n",
            "acc: 0.6052269601100413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gUtSBlCGCMD",
        "colab_type": "text"
      },
      "source": [
        "# BERT Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiVosUYqJabk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/uncased_L-12_H-768_A-12.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vApf1EAGCMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_offset_no_spaces(text, offset):\n",
        "    count = 0\n",
        "    for i in range(offset):\n",
        "        if text[i] != ' ': count += 1\n",
        "    return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPTMBcOEGCMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_length(text):\n",
        "    count = 0\n",
        "    for i in range(len(text)):\n",
        "        if text[i] not in ['#',' ']: count += 1\n",
        "    return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UyHnT2SGCMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_bert(data):\n",
        "    text = data[\"Text\"]\n",
        "    text.to_csv(\"input.txt\", index = False, header = False)\n",
        "    print('running bert')\n",
        "    os.system(\"python3 extract_features.py \\\n",
        "      --input_file=input.txt \\\n",
        "      --output_file=output.jsonl \\\n",
        "      --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n",
        "      --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n",
        "      --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
        "      --layers=-1 \\\n",
        "      --max_seq_length=256 \\\n",
        "      --batch_size=8\")\n",
        "    \n",
        "    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
        "\n",
        "    os.system(\"rm output.jsonl\")\n",
        "    os.system(\"rm input.txt\")\n",
        "    \n",
        "    print('post bert')\n",
        "\n",
        "    index = data.index\n",
        "    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
        "    emb = pd.DataFrame(index = index, columns = columns)\n",
        "    emb.index.name = \"ID\"\n",
        "    \n",
        "    for i in range(len(data)): \n",
        "        if i % 100 == 0: print(i)\n",
        "        P = data.loc[i,\"Pronoun\"].lower()\n",
        "        A = data.loc[i,\"A\"].lower()\n",
        "        B = data.loc[i,\"B\"].lower()\n",
        "\n",
        "        P_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n",
        "        A_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n",
        "        B_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n",
        "        A_length = count_length(A)\n",
        "        B_length = count_length(B)\n",
        "\n",
        "        emb_A = np.zeros(768)\n",
        "        emb_B = np.zeros(768)\n",
        "        emb_P = np.zeros(768)\n",
        "\n",
        "        count_chars = 0\n",
        "        cnt_A, cnt_B, cnt_P = 0, 0, 0\n",
        "\n",
        "        features = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n",
        "        for j in range(2,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n",
        "            token = features.loc[j,\"token\"]\n",
        "\n",
        "            if count_chars  == P_offset: \n",
        "                emb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n",
        "                cnt_P += 1\n",
        "            if count_chars in range(A_offset, A_offset + A_length): \n",
        "                emb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n",
        "                cnt_A +=1\n",
        "            if count_chars in range(B_offset, B_offset + B_length): \n",
        "                emb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n",
        "                cnt_B +=1\n",
        "            count_chars += count_length(token)\n",
        "        emb_A /= cnt_A\n",
        "        emb_B /= cnt_B\n",
        "\n",
        "        label = \"Neither\"\n",
        "        if (data.loc[i,\"A-coref\"] == True):\n",
        "            label = \"A\"\n",
        "        if (data.loc[i,\"B-coref\"] == True):\n",
        "            label = \"B\"\n",
        "\n",
        "        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n",
        "\n",
        "    return emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apmr--X0gtyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_json(embeddings):\n",
        "    embeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n",
        "    X = np.zeros((len(embeddings),3*768))\n",
        "    Y = np.zeros((len(embeddings), 3))\n",
        "\n",
        "    # Concatenate features\n",
        "    for i in range(len(embeddings)):\n",
        "        A = np.array(embeddings.loc[i,\"emb_A\"])\n",
        "        B = np.array(embeddings.loc[i,\"emb_B\"])\n",
        "        P = np.array(embeddings.loc[i,\"emb_P\"])\n",
        "        X[i] = np.concatenate((A,B,P))\n",
        "\n",
        "    # One-hot encoding for labels\n",
        "    for i in range(len(embeddings)):\n",
        "        label = embeddings.loc[i,\"label\"]\n",
        "        if label == \"A\":\n",
        "            Y[i,0] = 1\n",
        "        elif label == \"B\":\n",
        "            Y[i,1] = 1\n",
        "        else:\n",
        "            Y[i,2] = 1\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrUG_xeVGCMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb = pd.read_json('contextual_embeddings.json')\n",
        "emb = parse_json(emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtzHHlnvg4NC",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_X1Ad2qhhHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = emb[0][:2000]\n",
        "test_data = emb[0][2454:]\n",
        "val_data = emb[0][2000:2454]\n",
        "train_label = emb[1][:2000]\n",
        "test_label = emb[1][2454:]\n",
        "val_label = emb[1][2000:2454]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI-LEETTg2xI",
        "colab_type": "code",
        "outputId": "5fb3440a-a216-4de0-8b41-fdefa6cd0c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape, test_data.shape, val_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1998, 2304), (1999, 2304), (1999, 2304))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0O8JbZbhlHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image features\n",
        "inputs = Input(shape=(2304,))\n",
        "dense = Dense(256, activation='relu')(inputs)\n",
        "norm = BatchNormalization()(dense)\n",
        "dropout = Dropout(0.5)(norm)\n",
        "dense = Dense(3, activation='softmax')(dropout)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=dense)\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', kernel_regularizer = regularizers.l2(0.1))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBoMOEQwVt7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhtNoAnJkSCc",
        "colab_type": "code",
        "outputId": "0d021956-9a4f-4079-fd20-0eff696ce4bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x = train_data, y = train_label, epochs = 1000, batch_size = 32, validation_data = (val_data, val_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2000 samples, validate on 454 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2000/2000 [==============================] - 10s 5ms/step - loss: 1.0348 - val_loss: 0.9984\n",
            "Epoch 2/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9526 - val_loss: 1.0083\n",
            "Epoch 3/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9504 - val_loss: 1.0061\n",
            "Epoch 4/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9525 - val_loss: 1.0072\n",
            "Epoch 5/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9495 - val_loss: 1.0047\n",
            "Epoch 6/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9513 - val_loss: 1.0027\n",
            "Epoch 7/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9513 - val_loss: 1.0030\n",
            "Epoch 8/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9515 - val_loss: 1.0041\n",
            "Epoch 9/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0045\n",
            "Epoch 10/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9541 - val_loss: 1.0019\n",
            "Epoch 11/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9489 - val_loss: 1.0045\n",
            "Epoch 12/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9520 - val_loss: 1.0031\n",
            "Epoch 13/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9515 - val_loss: 1.0029\n",
            "Epoch 14/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9518 - val_loss: 1.0055\n",
            "Epoch 15/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9519 - val_loss: 1.0034\n",
            "Epoch 16/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9516 - val_loss: 1.0023\n",
            "Epoch 17/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9528 - val_loss: 1.0035\n",
            "Epoch 18/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9515 - val_loss: 1.0019\n",
            "Epoch 19/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9506 - val_loss: 1.0026\n",
            "Epoch 20/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9503 - val_loss: 1.0020\n",
            "Epoch 21/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9509 - val_loss: 1.0026\n",
            "Epoch 22/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9528 - val_loss: 1.0011\n",
            "Epoch 23/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9505 - val_loss: 1.0031\n",
            "Epoch 24/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9514 - val_loss: 1.0018\n",
            "Epoch 25/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9528 - val_loss: 1.0040\n",
            "Epoch 26/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9521 - val_loss: 1.0030\n",
            "Epoch 27/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9512 - val_loss: 1.0029\n",
            "Epoch 28/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9521 - val_loss: 1.0025\n",
            "Epoch 29/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9520 - val_loss: 1.0022\n",
            "Epoch 30/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9533 - val_loss: 1.0020\n",
            "Epoch 31/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9533 - val_loss: 1.0043\n",
            "Epoch 32/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9508 - val_loss: 1.0051\n",
            "Epoch 33/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0018\n",
            "Epoch 34/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9518 - val_loss: 1.0016\n",
            "Epoch 35/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9532 - val_loss: 1.0045\n",
            "Epoch 36/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0045\n",
            "Epoch 37/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9521 - val_loss: 1.0034\n",
            "Epoch 38/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9489 - val_loss: 1.0058\n",
            "Epoch 39/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9525 - val_loss: 1.0026\n",
            "Epoch 40/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9522 - val_loss: 1.0023\n",
            "Epoch 41/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9518 - val_loss: 1.0021\n",
            "Epoch 42/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0036\n",
            "Epoch 43/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9518 - val_loss: 1.0045\n",
            "Epoch 44/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9508 - val_loss: 1.0020\n",
            "Epoch 45/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9513 - val_loss: 1.0014\n",
            "Epoch 46/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9511 - val_loss: 1.0042\n",
            "Epoch 47/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9495 - val_loss: 1.0027\n",
            "Epoch 48/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9504 - val_loss: 1.0023\n",
            "Epoch 49/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9530 - val_loss: 1.0022\n",
            "Epoch 50/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9520 - val_loss: 1.0046\n",
            "Epoch 51/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9525 - val_loss: 1.0001\n",
            "Epoch 52/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9502 - val_loss: 1.0025\n",
            "Epoch 53/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0031\n",
            "Epoch 54/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9501 - val_loss: 1.0037\n",
            "Epoch 55/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9515 - val_loss: 1.0014\n",
            "Epoch 56/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9511 - val_loss: 1.0028\n",
            "Epoch 57/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9515 - val_loss: 1.0059\n",
            "Epoch 58/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9547 - val_loss: 1.0030\n",
            "Epoch 59/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9510 - val_loss: 1.0016\n",
            "Epoch 60/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9502 - val_loss: 0.9997\n",
            "Epoch 61/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9522 - val_loss: 1.0009\n",
            "Epoch 62/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9516 - val_loss: 1.0031\n",
            "Epoch 63/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9529 - val_loss: 1.0008\n",
            "Epoch 64/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9519 - val_loss: 1.0053\n",
            "Epoch 65/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9517 - val_loss: 1.0014\n",
            "Epoch 66/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9510 - val_loss: 1.0037\n",
            "Epoch 67/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9504 - val_loss: 1.0022\n",
            "Epoch 68/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9519 - val_loss: 1.0033\n",
            "Epoch 69/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0027\n",
            "Epoch 70/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9523 - val_loss: 1.0018\n",
            "Epoch 71/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9519 - val_loss: 1.0037\n",
            "Epoch 72/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9521 - val_loss: 1.0023\n",
            "Epoch 73/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9525 - val_loss: 1.0027\n",
            "Epoch 74/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9509 - val_loss: 1.0027\n",
            "Epoch 75/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9518 - val_loss: 1.0057\n",
            "Epoch 76/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9519 - val_loss: 1.0013\n",
            "Epoch 77/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9508 - val_loss: 1.0092\n",
            "Epoch 78/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9526 - val_loss: 1.0034\n",
            "Epoch 79/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9514 - val_loss: 1.0044\n",
            "Epoch 80/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9479 - val_loss: 1.0034\n",
            "Epoch 81/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9521 - val_loss: 1.0017\n",
            "Epoch 82/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9515 - val_loss: 1.0020\n",
            "Epoch 83/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9496 - val_loss: 1.0019\n",
            "Epoch 84/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9514 - val_loss: 1.0022\n",
            "Epoch 85/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0050\n",
            "Epoch 86/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9501 - val_loss: 1.0035\n",
            "Epoch 87/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9501 - val_loss: 1.0039\n",
            "Epoch 88/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9508 - val_loss: 1.0011\n",
            "Epoch 89/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9506 - val_loss: 1.0031\n",
            "Epoch 90/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9530 - val_loss: 0.9998\n",
            "Epoch 91/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9517 - val_loss: 1.0027\n",
            "Epoch 92/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9512 - val_loss: 1.0035\n",
            "Epoch 93/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9503 - val_loss: 1.0027\n",
            "Epoch 94/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9514 - val_loss: 1.0012\n",
            "Epoch 95/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9511 - val_loss: 1.0052\n",
            "Epoch 96/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9514 - val_loss: 1.0031\n",
            "Epoch 97/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9522 - val_loss: 1.0013\n",
            "Epoch 98/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9512 - val_loss: 1.0025\n",
            "Epoch 99/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9511 - val_loss: 1.0009\n",
            "Epoch 100/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9506 - val_loss: 1.0028\n",
            "Epoch 101/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9516 - val_loss: 1.0026\n",
            "Epoch 102/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9508 - val_loss: 1.0053\n",
            "Epoch 103/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9525 - val_loss: 1.0017\n",
            "Epoch 104/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9515 - val_loss: 0.9995\n",
            "Epoch 105/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0024\n",
            "Epoch 106/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9517 - val_loss: 1.0039\n",
            "Epoch 107/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9511 - val_loss: 1.0039\n",
            "Epoch 108/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9508 - val_loss: 1.0016\n",
            "Epoch 109/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9494 - val_loss: 1.0023\n",
            "Epoch 110/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9515 - val_loss: 1.0009\n",
            "Epoch 111/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9513 - val_loss: 1.0009\n",
            "Epoch 112/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9514 - val_loss: 1.0020\n",
            "Epoch 113/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9491 - val_loss: 1.0052\n",
            "Epoch 114/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9504 - val_loss: 1.0036\n",
            "Epoch 115/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9521 - val_loss: 1.0027\n",
            "Epoch 116/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9520 - val_loss: 1.0046\n",
            "Epoch 117/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9516 - val_loss: 1.0046\n",
            "Epoch 118/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9502 - val_loss: 1.0037\n",
            "Epoch 119/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9498 - val_loss: 1.0037\n",
            "Epoch 120/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9508 - val_loss: 1.0031\n",
            "Epoch 121/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9523 - val_loss: 1.0023\n",
            "Epoch 122/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9513 - val_loss: 1.0056\n",
            "Epoch 123/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0028\n",
            "Epoch 124/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9527 - val_loss: 1.0030\n",
            "Epoch 125/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9494 - val_loss: 1.0031\n",
            "Epoch 126/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0025\n",
            "Epoch 127/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0025\n",
            "Epoch 128/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9517 - val_loss: 1.0025\n",
            "Epoch 129/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9510 - val_loss: 1.0027\n",
            "Epoch 130/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0021\n",
            "Epoch 131/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9505 - val_loss: 1.0053\n",
            "Epoch 132/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9518 - val_loss: 1.0025\n",
            "Epoch 133/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9503 - val_loss: 1.0022\n",
            "Epoch 134/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9524 - val_loss: 1.0033\n",
            "Epoch 135/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9515 - val_loss: 1.0041\n",
            "Epoch 136/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9510 - val_loss: 1.0040\n",
            "Epoch 137/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9495 - val_loss: 1.0006\n",
            "Epoch 138/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9516 - val_loss: 1.0028\n",
            "Epoch 139/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9515 - val_loss: 1.0052\n",
            "Epoch 140/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9513 - val_loss: 1.0027\n",
            "Epoch 141/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0028\n",
            "Epoch 142/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9523 - val_loss: 1.0053\n",
            "Epoch 143/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9514 - val_loss: 0.9993\n",
            "Epoch 144/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9527 - val_loss: 1.0024\n",
            "Epoch 145/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9514 - val_loss: 1.0017\n",
            "Epoch 146/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9510 - val_loss: 1.0024\n",
            "Epoch 147/1000\n",
            "2000/2000 [==============================] - 0s 191us/step - loss: 0.9511 - val_loss: 1.0024\n",
            "Epoch 148/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9515 - val_loss: 1.0028\n",
            "Epoch 149/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9510 - val_loss: 1.0034\n",
            "Epoch 150/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9492 - val_loss: 1.0039\n",
            "Epoch 151/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9510 - val_loss: 1.0000\n",
            "Epoch 152/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9514 - val_loss: 1.0010\n",
            "Epoch 153/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9511 - val_loss: 1.0003\n",
            "Epoch 154/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9509 - val_loss: 1.0039\n",
            "Epoch 155/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9511 - val_loss: 1.0034\n",
            "Epoch 156/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9498 - val_loss: 1.0026\n",
            "Epoch 157/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9512 - val_loss: 1.0016\n",
            "Epoch 158/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9515 - val_loss: 1.0049\n",
            "Epoch 159/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9509 - val_loss: 1.0014\n",
            "Epoch 160/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9513 - val_loss: 1.0037\n",
            "Epoch 161/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9519 - val_loss: 1.0034\n",
            "Epoch 162/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9509 - val_loss: 1.0012\n",
            "Epoch 163/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9508 - val_loss: 1.0028\n",
            "Epoch 164/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9527 - val_loss: 1.0022\n",
            "Epoch 165/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9511 - val_loss: 0.9992\n",
            "Epoch 166/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9522 - val_loss: 1.0021\n",
            "Epoch 167/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9522 - val_loss: 1.0019\n",
            "Epoch 168/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9506 - val_loss: 1.0028\n",
            "Epoch 169/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9502 - val_loss: 1.0024\n",
            "Epoch 170/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9511 - val_loss: 1.0003\n",
            "Epoch 171/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9507 - val_loss: 1.0027\n",
            "Epoch 172/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9495 - val_loss: 1.0060\n",
            "Epoch 173/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9511 - val_loss: 1.0020\n",
            "Epoch 174/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9525 - val_loss: 1.0020\n",
            "Epoch 175/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9522 - val_loss: 0.9998\n",
            "Epoch 176/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9505 - val_loss: 1.0052\n",
            "Epoch 177/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9507 - val_loss: 1.0039\n",
            "Epoch 178/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9509 - val_loss: 1.0030\n",
            "Epoch 179/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9501 - val_loss: 1.0014\n",
            "Epoch 180/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9518 - val_loss: 1.0020\n",
            "Epoch 181/1000\n",
            "2000/2000 [==============================] - 0s 175us/step - loss: 0.9495 - val_loss: 1.0028\n",
            "Epoch 182/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9511 - val_loss: 1.0021\n",
            "Epoch 183/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9509 - val_loss: 1.0051\n",
            "Epoch 184/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9514 - val_loss: 1.0055\n",
            "Epoch 185/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9495 - val_loss: 1.0050\n",
            "Epoch 186/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9515 - val_loss: 1.0009\n",
            "Epoch 187/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9507 - val_loss: 1.0045\n",
            "Epoch 188/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0003\n",
            "Epoch 189/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9509 - val_loss: 1.0017\n",
            "Epoch 190/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9517 - val_loss: 1.0026\n",
            "Epoch 191/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9521 - val_loss: 1.0021\n",
            "Epoch 192/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9514 - val_loss: 1.0051\n",
            "Epoch 193/1000\n",
            "2000/2000 [==============================] - 0s 193us/step - loss: 0.9513 - val_loss: 1.0015\n",
            "Epoch 194/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9511 - val_loss: 1.0029\n",
            "Epoch 195/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9504 - val_loss: 1.0046\n",
            "Epoch 196/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9506 - val_loss: 1.0015\n",
            "Epoch 197/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0013\n",
            "Epoch 198/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0031\n",
            "Epoch 199/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9505 - val_loss: 1.0029\n",
            "Epoch 200/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9520 - val_loss: 1.0011\n",
            "Epoch 201/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9499 - val_loss: 1.0057\n",
            "Epoch 202/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9518 - val_loss: 1.0021\n",
            "Epoch 203/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9509 - val_loss: 1.0013\n",
            "Epoch 204/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9524 - val_loss: 1.0010\n",
            "Epoch 205/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9521 - val_loss: 1.0057\n",
            "Epoch 206/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9517 - val_loss: 1.0004\n",
            "Epoch 207/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0033\n",
            "Epoch 208/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9504 - val_loss: 1.0023\n",
            "Epoch 209/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9504 - val_loss: 1.0008\n",
            "Epoch 210/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9528 - val_loss: 1.0007\n",
            "Epoch 211/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9505 - val_loss: 1.0030\n",
            "Epoch 212/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9505 - val_loss: 1.0029\n",
            "Epoch 213/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9510 - val_loss: 1.0053\n",
            "Epoch 214/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0042\n",
            "Epoch 215/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9508 - val_loss: 1.0027\n",
            "Epoch 216/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9506 - val_loss: 1.0021\n",
            "Epoch 217/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9496 - val_loss: 1.0019\n",
            "Epoch 218/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9516 - val_loss: 1.0022\n",
            "Epoch 219/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9508 - val_loss: 1.0029\n",
            "Epoch 220/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9509 - val_loss: 1.0036\n",
            "Epoch 221/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9499 - val_loss: 1.0024\n",
            "Epoch 222/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9511 - val_loss: 1.0027\n",
            "Epoch 223/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9500 - val_loss: 1.0032\n",
            "Epoch 224/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9507 - val_loss: 1.0025\n",
            "Epoch 225/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9503 - val_loss: 1.0007\n",
            "Epoch 226/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9510 - val_loss: 1.0071\n",
            "Epoch 227/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9516 - val_loss: 1.0032\n",
            "Epoch 228/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9516 - val_loss: 1.0016\n",
            "Epoch 229/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9523 - val_loss: 1.0013\n",
            "Epoch 230/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9501 - val_loss: 1.0012\n",
            "Epoch 231/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9499 - val_loss: 1.0051\n",
            "Epoch 232/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9529 - val_loss: 1.0026\n",
            "Epoch 233/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0009\n",
            "Epoch 234/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9518 - val_loss: 1.0003\n",
            "Epoch 235/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9500 - val_loss: 1.0053\n",
            "Epoch 236/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9496 - val_loss: 1.0048\n",
            "Epoch 237/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9513 - val_loss: 1.0028\n",
            "Epoch 238/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9513 - val_loss: 1.0031\n",
            "Epoch 239/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9503 - val_loss: 1.0007\n",
            "Epoch 240/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9509 - val_loss: 1.0043\n",
            "Epoch 241/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9508 - val_loss: 1.0023\n",
            "Epoch 242/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9496 - val_loss: 1.0024\n",
            "Epoch 243/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9508 - val_loss: 1.0022\n",
            "Epoch 244/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9510 - val_loss: 1.0031\n",
            "Epoch 245/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0046\n",
            "Epoch 246/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9529 - val_loss: 1.0031\n",
            "Epoch 247/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9513 - val_loss: 1.0058\n",
            "Epoch 248/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9495 - val_loss: 1.0030\n",
            "Epoch 249/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9488 - val_loss: 1.0029\n",
            "Epoch 250/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9519 - val_loss: 1.0022\n",
            "Epoch 251/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0018\n",
            "Epoch 252/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9497 - val_loss: 1.0026\n",
            "Epoch 253/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9499 - val_loss: 1.0037\n",
            "Epoch 254/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9510 - val_loss: 1.0022\n",
            "Epoch 255/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9507 - val_loss: 1.0026\n",
            "Epoch 256/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9517 - val_loss: 1.0012\n",
            "Epoch 257/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9510 - val_loss: 1.0041\n",
            "Epoch 258/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9499 - val_loss: 1.0045\n",
            "Epoch 259/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9495 - val_loss: 1.0042\n",
            "Epoch 260/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9516 - val_loss: 1.0024\n",
            "Epoch 261/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0020\n",
            "Epoch 262/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9514 - val_loss: 1.0029\n",
            "Epoch 263/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9526 - val_loss: 1.0024\n",
            "Epoch 264/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9492 - val_loss: 1.0033\n",
            "Epoch 265/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9500 - val_loss: 1.0039\n",
            "Epoch 266/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9515 - val_loss: 1.0023\n",
            "Epoch 267/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9509 - val_loss: 1.0036\n",
            "Epoch 268/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9495 - val_loss: 1.0012\n",
            "Epoch 269/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9511 - val_loss: 1.0024\n",
            "Epoch 270/1000\n",
            "2000/2000 [==============================] - 0s 175us/step - loss: 0.9496 - val_loss: 1.0039\n",
            "Epoch 271/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9508 - val_loss: 1.0045\n",
            "Epoch 272/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9516 - val_loss: 1.0041\n",
            "Epoch 273/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9510 - val_loss: 1.0023\n",
            "Epoch 274/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9492 - val_loss: 1.0030\n",
            "Epoch 275/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9517 - val_loss: 1.0018\n",
            "Epoch 276/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9500 - val_loss: 1.0016\n",
            "Epoch 277/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9499 - val_loss: 1.0018\n",
            "Epoch 278/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9513 - val_loss: 1.0031\n",
            "Epoch 279/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9511 - val_loss: 1.0039\n",
            "Epoch 280/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9516 - val_loss: 1.0044\n",
            "Epoch 281/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9525 - val_loss: 1.0005\n",
            "Epoch 282/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9499 - val_loss: 1.0032\n",
            "Epoch 283/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9510 - val_loss: 1.0021\n",
            "Epoch 284/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9515 - val_loss: 1.0045\n",
            "Epoch 285/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9506 - val_loss: 1.0041\n",
            "Epoch 286/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9519 - val_loss: 1.0035\n",
            "Epoch 287/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9512 - val_loss: 1.0034\n",
            "Epoch 288/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9511 - val_loss: 1.0005\n",
            "Epoch 289/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9495 - val_loss: 1.0009\n",
            "Epoch 290/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9503 - val_loss: 1.0018\n",
            "Epoch 291/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9510 - val_loss: 1.0026\n",
            "Epoch 292/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9505 - val_loss: 1.0017\n",
            "Epoch 293/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9502 - val_loss: 1.0034\n",
            "Epoch 294/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9516 - val_loss: 1.0043\n",
            "Epoch 295/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9513 - val_loss: 1.0011\n",
            "Epoch 296/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9519 - val_loss: 1.0008\n",
            "Epoch 297/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9509 - val_loss: 1.0025\n",
            "Epoch 298/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9514 - val_loss: 1.0027\n",
            "Epoch 299/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9511 - val_loss: 1.0023\n",
            "Epoch 300/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0020\n",
            "Epoch 301/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0034\n",
            "Epoch 302/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0021\n",
            "Epoch 303/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9518 - val_loss: 1.0060\n",
            "Epoch 304/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0016\n",
            "Epoch 305/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9494 - val_loss: 1.0024\n",
            "Epoch 306/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9513 - val_loss: 1.0035\n",
            "Epoch 307/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0023\n",
            "Epoch 308/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0011\n",
            "Epoch 309/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0039\n",
            "Epoch 310/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9490 - val_loss: 1.0020\n",
            "Epoch 311/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9521 - val_loss: 1.0012\n",
            "Epoch 312/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9507 - val_loss: 1.0042\n",
            "Epoch 313/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9514 - val_loss: 1.0032\n",
            "Epoch 314/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9507 - val_loss: 1.0015\n",
            "Epoch 315/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9503 - val_loss: 1.0030\n",
            "Epoch 316/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9502 - val_loss: 1.0020\n",
            "Epoch 317/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9504 - val_loss: 1.0008\n",
            "Epoch 318/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9511 - val_loss: 1.0040\n",
            "Epoch 319/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9508 - val_loss: 1.0055\n",
            "Epoch 320/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9501 - val_loss: 1.0029\n",
            "Epoch 321/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9499 - val_loss: 1.0035\n",
            "Epoch 322/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9520 - val_loss: 1.0023\n",
            "Epoch 323/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9486 - val_loss: 1.0040\n",
            "Epoch 324/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0030\n",
            "Epoch 325/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9506 - val_loss: 1.0025\n",
            "Epoch 326/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9512 - val_loss: 1.0043\n",
            "Epoch 327/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9507 - val_loss: 1.0047\n",
            "Epoch 328/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0029\n",
            "Epoch 329/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9519 - val_loss: 1.0026\n",
            "Epoch 330/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0038\n",
            "Epoch 331/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9510 - val_loss: 1.0030\n",
            "Epoch 332/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9513 - val_loss: 1.0013\n",
            "Epoch 333/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9522 - val_loss: 1.0026\n",
            "Epoch 334/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9502 - val_loss: 1.0024\n",
            "Epoch 335/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9514 - val_loss: 1.0020\n",
            "Epoch 336/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9513 - val_loss: 1.0030\n",
            "Epoch 337/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0033\n",
            "Epoch 338/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9511 - val_loss: 1.0022\n",
            "Epoch 339/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9505 - val_loss: 1.0021\n",
            "Epoch 340/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9519 - val_loss: 1.0028\n",
            "Epoch 341/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9503 - val_loss: 1.0044\n",
            "Epoch 342/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9515 - val_loss: 1.0025\n",
            "Epoch 343/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9521 - val_loss: 1.0004\n",
            "Epoch 344/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9512 - val_loss: 1.0021\n",
            "Epoch 345/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9502 - val_loss: 1.0027\n",
            "Epoch 346/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9493 - val_loss: 1.0023\n",
            "Epoch 347/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0026\n",
            "Epoch 348/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9516 - val_loss: 1.0011\n",
            "Epoch 349/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9503 - val_loss: 1.0039\n",
            "Epoch 350/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9521 - val_loss: 1.0002\n",
            "Epoch 351/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9500 - val_loss: 1.0021\n",
            "Epoch 352/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9518 - val_loss: 1.0029\n",
            "Epoch 353/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9495 - val_loss: 1.0019\n",
            "Epoch 354/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9506 - val_loss: 1.0015\n",
            "Epoch 355/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9501 - val_loss: 1.0018\n",
            "Epoch 356/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9497 - val_loss: 1.0032\n",
            "Epoch 357/1000\n",
            "2000/2000 [==============================] - 0s 194us/step - loss: 0.9508 - val_loss: 1.0030\n",
            "Epoch 358/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9498 - val_loss: 1.0047\n",
            "Epoch 359/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9516 - val_loss: 1.0046\n",
            "Epoch 360/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9512 - val_loss: 1.0013\n",
            "Epoch 361/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9499 - val_loss: 1.0022\n",
            "Epoch 362/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9497 - val_loss: 1.0018\n",
            "Epoch 363/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9509 - val_loss: 1.0013\n",
            "Epoch 364/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0036\n",
            "Epoch 365/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9515 - val_loss: 1.0019\n",
            "Epoch 366/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9496 - val_loss: 1.0036\n",
            "Epoch 367/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9510 - val_loss: 1.0031\n",
            "Epoch 368/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9505 - val_loss: 1.0026\n",
            "Epoch 369/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9496 - val_loss: 1.0040\n",
            "Epoch 370/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9512 - val_loss: 1.0022\n",
            "Epoch 371/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9510 - val_loss: 1.0033\n",
            "Epoch 372/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9514 - val_loss: 1.0012\n",
            "Epoch 373/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9498 - val_loss: 1.0027\n",
            "Epoch 374/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9492 - val_loss: 1.0020\n",
            "Epoch 375/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9509 - val_loss: 1.0034\n",
            "Epoch 376/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9511 - val_loss: 1.0034\n",
            "Epoch 377/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9509 - val_loss: 1.0031\n",
            "Epoch 378/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9501 - val_loss: 1.0028\n",
            "Epoch 379/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9506 - val_loss: 1.0039\n",
            "Epoch 380/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9507 - val_loss: 1.0038\n",
            "Epoch 381/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9507 - val_loss: 1.0017\n",
            "Epoch 382/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9511 - val_loss: 1.0019\n",
            "Epoch 383/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9525 - val_loss: 1.0034\n",
            "Epoch 384/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9502 - val_loss: 1.0038\n",
            "Epoch 385/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0017\n",
            "Epoch 386/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0041\n",
            "Epoch 387/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9518 - val_loss: 1.0019\n",
            "Epoch 388/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9513 - val_loss: 1.0016\n",
            "Epoch 389/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9507 - val_loss: 1.0039\n",
            "Epoch 390/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9493 - val_loss: 1.0033\n",
            "Epoch 391/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9512 - val_loss: 1.0016\n",
            "Epoch 392/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9498 - val_loss: 1.0021\n",
            "Epoch 393/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9508 - val_loss: 1.0017\n",
            "Epoch 394/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9507 - val_loss: 1.0037\n",
            "Epoch 395/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0022\n",
            "Epoch 396/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9509 - val_loss: 1.0017\n",
            "Epoch 397/1000\n",
            "2000/2000 [==============================] - 0s 193us/step - loss: 0.9512 - val_loss: 1.0039\n",
            "Epoch 398/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0014\n",
            "Epoch 399/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9504 - val_loss: 1.0017\n",
            "Epoch 400/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9493 - val_loss: 1.0041\n",
            "Epoch 401/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9491 - val_loss: 1.0037\n",
            "Epoch 402/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9512 - val_loss: 1.0040\n",
            "Epoch 403/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9509 - val_loss: 1.0052\n",
            "Epoch 404/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9503 - val_loss: 1.0036\n",
            "Epoch 405/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9506 - val_loss: 1.0027\n",
            "Epoch 406/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9515 - val_loss: 1.0011\n",
            "Epoch 407/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9504 - val_loss: 1.0041\n",
            "Epoch 408/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9518 - val_loss: 1.0024\n",
            "Epoch 409/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9505 - val_loss: 1.0029\n",
            "Epoch 410/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9508 - val_loss: 1.0051\n",
            "Epoch 411/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9491 - val_loss: 1.0026\n",
            "Epoch 412/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9501 - val_loss: 1.0029\n",
            "Epoch 413/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0022\n",
            "Epoch 414/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0024\n",
            "Epoch 415/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9503 - val_loss: 1.0024\n",
            "Epoch 416/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9502 - val_loss: 1.0018\n",
            "Epoch 417/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9521 - val_loss: 1.0035\n",
            "Epoch 418/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9504 - val_loss: 1.0035\n",
            "Epoch 419/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9503 - val_loss: 1.0038\n",
            "Epoch 420/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9511 - val_loss: 1.0032\n",
            "Epoch 421/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9519 - val_loss: 1.0030\n",
            "Epoch 422/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9506 - val_loss: 1.0037\n",
            "Epoch 423/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9516 - val_loss: 1.0034\n",
            "Epoch 424/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9505 - val_loss: 1.0047\n",
            "Epoch 425/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9501 - val_loss: 1.0014\n",
            "Epoch 426/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9503 - val_loss: 1.0025\n",
            "Epoch 427/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9507 - val_loss: 1.0034\n",
            "Epoch 428/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9509 - val_loss: 1.0021\n",
            "Epoch 429/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9505 - val_loss: 1.0026\n",
            "Epoch 430/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9496 - val_loss: 1.0021\n",
            "Epoch 431/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9498 - val_loss: 1.0019\n",
            "Epoch 432/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0041\n",
            "Epoch 433/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0021\n",
            "Epoch 434/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9500 - val_loss: 1.0027\n",
            "Epoch 435/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9509 - val_loss: 1.0021\n",
            "Epoch 436/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9496 - val_loss: 1.0045\n",
            "Epoch 437/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9492 - val_loss: 1.0036\n",
            "Epoch 438/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9507 - val_loss: 1.0018\n",
            "Epoch 439/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9495 - val_loss: 1.0031\n",
            "Epoch 440/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9500 - val_loss: 1.0023\n",
            "Epoch 441/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9503 - val_loss: 1.0024\n",
            "Epoch 442/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9504 - val_loss: 1.0037\n",
            "Epoch 443/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9512 - val_loss: 1.0031\n",
            "Epoch 444/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0028\n",
            "Epoch 445/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9502 - val_loss: 1.0031\n",
            "Epoch 446/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9497 - val_loss: 1.0007\n",
            "Epoch 447/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9504 - val_loss: 1.0007\n",
            "Epoch 448/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9498 - val_loss: 1.0016\n",
            "Epoch 449/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9500 - val_loss: 1.0014\n",
            "Epoch 450/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9520 - val_loss: 1.0007\n",
            "Epoch 451/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9521 - val_loss: 1.0030\n",
            "Epoch 452/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9505 - val_loss: 1.0013\n",
            "Epoch 453/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0026\n",
            "Epoch 454/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9516 - val_loss: 1.0012\n",
            "Epoch 455/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9504 - val_loss: 1.0025\n",
            "Epoch 456/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9506 - val_loss: 1.0041\n",
            "Epoch 457/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9499 - val_loss: 1.0023\n",
            "Epoch 458/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9506 - val_loss: 1.0045\n",
            "Epoch 459/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9497 - val_loss: 1.0036\n",
            "Epoch 460/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9510 - val_loss: 1.0015\n",
            "Epoch 461/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9511 - val_loss: 1.0015\n",
            "Epoch 462/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9510 - val_loss: 1.0025\n",
            "Epoch 463/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9509 - val_loss: 1.0015\n",
            "Epoch 464/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9514 - val_loss: 1.0027\n",
            "Epoch 465/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9520 - val_loss: 1.0010\n",
            "Epoch 466/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9507 - val_loss: 1.0032\n",
            "Epoch 467/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9512 - val_loss: 1.0026\n",
            "Epoch 468/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0029\n",
            "Epoch 469/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9501 - val_loss: 1.0031\n",
            "Epoch 470/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9504 - val_loss: 1.0024\n",
            "Epoch 471/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9513 - val_loss: 1.0016\n",
            "Epoch 472/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9509 - val_loss: 1.0020\n",
            "Epoch 473/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9508 - val_loss: 1.0023\n",
            "Epoch 474/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9509 - val_loss: 1.0015\n",
            "Epoch 475/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9515 - val_loss: 1.0017\n",
            "Epoch 476/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9493 - val_loss: 1.0024\n",
            "Epoch 477/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9510 - val_loss: 1.0031\n",
            "Epoch 478/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9512 - val_loss: 1.0014\n",
            "Epoch 479/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9503 - val_loss: 1.0020\n",
            "Epoch 480/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9498 - val_loss: 1.0023\n",
            "Epoch 481/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0053\n",
            "Epoch 482/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0038\n",
            "Epoch 483/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9509 - val_loss: 1.0032\n",
            "Epoch 484/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9492 - val_loss: 1.0043\n",
            "Epoch 485/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0017\n",
            "Epoch 486/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9517 - val_loss: 1.0022\n",
            "Epoch 487/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9504 - val_loss: 1.0024\n",
            "Epoch 488/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9510 - val_loss: 1.0030\n",
            "Epoch 489/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9507 - val_loss: 1.0020\n",
            "Epoch 490/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9503 - val_loss: 1.0034\n",
            "Epoch 491/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9501 - val_loss: 1.0026\n",
            "Epoch 492/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9511 - val_loss: 1.0030\n",
            "Epoch 493/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9491 - val_loss: 1.0020\n",
            "Epoch 494/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9506 - val_loss: 1.0032\n",
            "Epoch 495/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9508 - val_loss: 1.0027\n",
            "Epoch 496/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9505 - val_loss: 1.0016\n",
            "Epoch 497/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9504 - val_loss: 1.0017\n",
            "Epoch 498/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9506 - val_loss: 1.0031\n",
            "Epoch 499/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9499 - val_loss: 1.0030\n",
            "Epoch 500/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0017\n",
            "Epoch 501/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9514 - val_loss: 1.0014\n",
            "Epoch 502/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9499 - val_loss: 1.0041\n",
            "Epoch 503/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9505 - val_loss: 1.0028\n",
            "Epoch 504/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9513 - val_loss: 1.0024\n",
            "Epoch 505/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0028\n",
            "Epoch 506/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9503 - val_loss: 1.0029\n",
            "Epoch 507/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9501 - val_loss: 1.0029\n",
            "Epoch 508/1000\n",
            "2000/2000 [==============================] - 0s 175us/step - loss: 0.9496 - val_loss: 1.0026\n",
            "Epoch 509/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9499 - val_loss: 1.0031\n",
            "Epoch 510/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9505 - val_loss: 1.0039\n",
            "Epoch 511/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9504 - val_loss: 1.0032\n",
            "Epoch 512/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0018\n",
            "Epoch 513/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9497 - val_loss: 1.0030\n",
            "Epoch 514/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9498 - val_loss: 1.0031\n",
            "Epoch 515/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9503 - val_loss: 1.0016\n",
            "Epoch 516/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9497 - val_loss: 1.0022\n",
            "Epoch 517/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9499 - val_loss: 1.0029\n",
            "Epoch 518/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9497 - val_loss: 1.0034\n",
            "Epoch 519/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9492 - val_loss: 1.0022\n",
            "Epoch 520/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9506 - val_loss: 1.0031\n",
            "Epoch 521/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9501 - val_loss: 1.0017\n",
            "Epoch 522/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9501 - val_loss: 1.0021\n",
            "Epoch 523/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9513 - val_loss: 1.0028\n",
            "Epoch 524/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9508 - val_loss: 1.0035\n",
            "Epoch 525/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9503 - val_loss: 1.0032\n",
            "Epoch 526/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9512 - val_loss: 1.0020\n",
            "Epoch 527/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9506 - val_loss: 1.0036\n",
            "Epoch 528/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9498 - val_loss: 1.0035\n",
            "Epoch 529/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9508 - val_loss: 1.0040\n",
            "Epoch 530/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0036\n",
            "Epoch 531/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9505 - val_loss: 1.0049\n",
            "Epoch 532/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0030\n",
            "Epoch 533/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0034\n",
            "Epoch 534/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9508 - val_loss: 1.0029\n",
            "Epoch 535/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9495 - val_loss: 1.0025\n",
            "Epoch 536/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9499 - val_loss: 1.0037\n",
            "Epoch 537/1000\n",
            "2000/2000 [==============================] - 0s 191us/step - loss: 0.9503 - val_loss: 1.0030\n",
            "Epoch 538/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9510 - val_loss: 1.0038\n",
            "Epoch 539/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9504 - val_loss: 1.0029\n",
            "Epoch 540/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9503 - val_loss: 1.0027\n",
            "Epoch 541/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9508 - val_loss: 1.0017\n",
            "Epoch 542/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9491 - val_loss: 1.0033\n",
            "Epoch 543/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9513 - val_loss: 1.0024\n",
            "Epoch 544/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9507 - val_loss: 1.0037\n",
            "Epoch 545/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9515 - val_loss: 1.0011\n",
            "Epoch 546/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9503 - val_loss: 1.0023\n",
            "Epoch 547/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9506 - val_loss: 1.0027\n",
            "Epoch 548/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9518 - val_loss: 1.0031\n",
            "Epoch 549/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9505 - val_loss: 1.0046\n",
            "Epoch 550/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9491 - val_loss: 1.0019\n",
            "Epoch 551/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9504 - val_loss: 1.0030\n",
            "Epoch 552/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9500 - val_loss: 1.0023\n",
            "Epoch 553/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9495 - val_loss: 1.0026\n",
            "Epoch 554/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9499 - val_loss: 1.0032\n",
            "Epoch 555/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9502 - val_loss: 1.0019\n",
            "Epoch 556/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9507 - val_loss: 1.0027\n",
            "Epoch 557/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9495 - val_loss: 1.0027\n",
            "Epoch 558/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9504 - val_loss: 1.0014\n",
            "Epoch 559/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0015\n",
            "Epoch 560/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9506 - val_loss: 1.0026\n",
            "Epoch 561/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9499 - val_loss: 1.0022\n",
            "Epoch 562/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9503 - val_loss: 1.0023\n",
            "Epoch 563/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9493 - val_loss: 1.0021\n",
            "Epoch 564/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0033\n",
            "Epoch 565/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9494 - val_loss: 1.0026\n",
            "Epoch 566/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9507 - val_loss: 1.0026\n",
            "Epoch 567/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9506 - val_loss: 1.0016\n",
            "Epoch 568/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9496 - val_loss: 1.0038\n",
            "Epoch 569/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9515 - val_loss: 1.0035\n",
            "Epoch 570/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9495 - val_loss: 1.0026\n",
            "Epoch 571/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9502 - val_loss: 1.0034\n",
            "Epoch 572/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9504 - val_loss: 1.0015\n",
            "Epoch 573/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9514 - val_loss: 1.0024\n",
            "Epoch 574/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0022\n",
            "Epoch 575/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9507 - val_loss: 1.0012\n",
            "Epoch 576/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9502 - val_loss: 1.0037\n",
            "Epoch 577/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9508 - val_loss: 1.0024\n",
            "Epoch 578/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9510 - val_loss: 1.0034\n",
            "Epoch 579/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9517 - val_loss: 1.0017\n",
            "Epoch 580/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0034\n",
            "Epoch 581/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9499 - val_loss: 1.0032\n",
            "Epoch 582/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9519 - val_loss: 1.0016\n",
            "Epoch 583/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9512 - val_loss: 1.0019\n",
            "Epoch 584/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9489 - val_loss: 1.0028\n",
            "Epoch 585/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9500 - val_loss: 1.0043\n",
            "Epoch 586/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9510 - val_loss: 1.0047\n",
            "Epoch 587/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9504 - val_loss: 1.0052\n",
            "Epoch 588/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9510 - val_loss: 1.0020\n",
            "Epoch 589/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9504 - val_loss: 1.0022\n",
            "Epoch 590/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9500 - val_loss: 1.0011\n",
            "Epoch 591/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0024\n",
            "Epoch 592/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9501 - val_loss: 1.0013\n",
            "Epoch 593/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9506 - val_loss: 1.0026\n",
            "Epoch 594/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9494 - val_loss: 1.0035\n",
            "Epoch 595/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9498 - val_loss: 1.0030\n",
            "Epoch 596/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0027\n",
            "Epoch 597/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9511 - val_loss: 1.0027\n",
            "Epoch 598/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9500 - val_loss: 1.0017\n",
            "Epoch 599/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9512 - val_loss: 1.0017\n",
            "Epoch 600/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9515 - val_loss: 1.0024\n",
            "Epoch 601/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9512 - val_loss: 1.0024\n",
            "Epoch 602/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0025\n",
            "Epoch 603/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9501 - val_loss: 1.0025\n",
            "Epoch 604/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9500 - val_loss: 1.0027\n",
            "Epoch 605/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9501 - val_loss: 1.0026\n",
            "Epoch 606/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0015\n",
            "Epoch 607/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9507 - val_loss: 1.0030\n",
            "Epoch 608/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0020\n",
            "Epoch 609/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9505 - val_loss: 1.0029\n",
            "Epoch 610/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9500 - val_loss: 1.0030\n",
            "Epoch 611/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9500 - val_loss: 1.0019\n",
            "Epoch 612/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9505 - val_loss: 1.0022\n",
            "Epoch 613/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9507 - val_loss: 1.0013\n",
            "Epoch 614/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9506 - val_loss: 1.0031\n",
            "Epoch 615/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9509 - val_loss: 1.0029\n",
            "Epoch 616/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9501 - val_loss: 1.0023\n",
            "Epoch 617/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9503 - val_loss: 1.0017\n",
            "Epoch 618/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9510 - val_loss: 1.0030\n",
            "Epoch 619/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9499 - val_loss: 1.0021\n",
            "Epoch 620/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9506 - val_loss: 1.0023\n",
            "Epoch 621/1000\n",
            "2000/2000 [==============================] - 0s 173us/step - loss: 0.9505 - val_loss: 1.0022\n",
            "Epoch 622/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9487 - val_loss: 1.0027\n",
            "Epoch 623/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0024\n",
            "Epoch 624/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9498 - val_loss: 1.0016\n",
            "Epoch 625/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0030\n",
            "Epoch 626/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0033\n",
            "Epoch 627/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9495 - val_loss: 1.0036\n",
            "Epoch 628/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9498 - val_loss: 1.0033\n",
            "Epoch 629/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9502 - val_loss: 1.0036\n",
            "Epoch 630/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9513 - val_loss: 1.0024\n",
            "Epoch 631/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9513 - val_loss: 1.0041\n",
            "Epoch 632/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9506 - val_loss: 1.0027\n",
            "Epoch 633/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9498 - val_loss: 1.0036\n",
            "Epoch 634/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0030\n",
            "Epoch 635/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9505 - val_loss: 1.0027\n",
            "Epoch 636/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9497 - val_loss: 1.0022\n",
            "Epoch 637/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9512 - val_loss: 1.0033\n",
            "Epoch 638/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0038\n",
            "Epoch 639/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9502 - val_loss: 1.0044\n",
            "Epoch 640/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9499 - val_loss: 1.0036\n",
            "Epoch 641/1000\n",
            "2000/2000 [==============================] - 0s 175us/step - loss: 0.9501 - val_loss: 1.0023\n",
            "Epoch 642/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9496 - val_loss: 1.0027\n",
            "Epoch 643/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0045\n",
            "Epoch 644/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9507 - val_loss: 1.0037\n",
            "Epoch 645/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9500 - val_loss: 1.0027\n",
            "Epoch 646/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9514 - val_loss: 1.0031\n",
            "Epoch 647/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9512 - val_loss: 1.0019\n",
            "Epoch 648/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9502 - val_loss: 1.0026\n",
            "Epoch 649/1000\n",
            "2000/2000 [==============================] - 0s 195us/step - loss: 0.9500 - val_loss: 1.0027\n",
            "Epoch 650/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9505 - val_loss: 1.0018\n",
            "Epoch 651/1000\n",
            "2000/2000 [==============================] - 0s 192us/step - loss: 0.9498 - val_loss: 1.0033\n",
            "Epoch 652/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9507 - val_loss: 1.0031\n",
            "Epoch 653/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9506 - val_loss: 1.0026\n",
            "Epoch 654/1000\n",
            "2000/2000 [==============================] - 0s 195us/step - loss: 0.9503 - val_loss: 1.0019\n",
            "Epoch 655/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9509 - val_loss: 1.0035\n",
            "Epoch 656/1000\n",
            "2000/2000 [==============================] - 0s 191us/step - loss: 0.9501 - val_loss: 1.0024\n",
            "Epoch 657/1000\n",
            "2000/2000 [==============================] - 0s 195us/step - loss: 0.9500 - val_loss: 1.0031\n",
            "Epoch 658/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9500 - val_loss: 1.0037\n",
            "Epoch 659/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9510 - val_loss: 1.0026\n",
            "Epoch 660/1000\n",
            "2000/2000 [==============================] - 0s 196us/step - loss: 0.9502 - val_loss: 1.0029\n",
            "Epoch 661/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9505 - val_loss: 1.0034\n",
            "Epoch 662/1000\n",
            "2000/2000 [==============================] - 0s 191us/step - loss: 0.9506 - val_loss: 1.0024\n",
            "Epoch 663/1000\n",
            "2000/2000 [==============================] - 0s 193us/step - loss: 0.9500 - val_loss: 1.0026\n",
            "Epoch 664/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9496 - val_loss: 1.0040\n",
            "Epoch 665/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9497 - val_loss: 1.0033\n",
            "Epoch 666/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9502 - val_loss: 1.0027\n",
            "Epoch 667/1000\n",
            "2000/2000 [==============================] - 0s 194us/step - loss: 0.9513 - val_loss: 1.0013\n",
            "Epoch 668/1000\n",
            "2000/2000 [==============================] - 0s 193us/step - loss: 0.9501 - val_loss: 1.0019\n",
            "Epoch 669/1000\n",
            "2000/2000 [==============================] - 0s 194us/step - loss: 0.9499 - val_loss: 1.0033\n",
            "Epoch 670/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9503 - val_loss: 1.0040\n",
            "Epoch 671/1000\n",
            "2000/2000 [==============================] - 0s 191us/step - loss: 0.9503 - val_loss: 1.0031\n",
            "Epoch 672/1000\n",
            "2000/2000 [==============================] - 0s 199us/step - loss: 0.9507 - val_loss: 1.0022\n",
            "Epoch 673/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9505 - val_loss: 1.0015\n",
            "Epoch 674/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9507 - val_loss: 1.0025\n",
            "Epoch 675/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9495 - val_loss: 1.0029\n",
            "Epoch 676/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0020\n",
            "Epoch 677/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9496 - val_loss: 1.0018\n",
            "Epoch 678/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0024\n",
            "Epoch 679/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9501 - val_loss: 1.0023\n",
            "Epoch 680/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9496 - val_loss: 1.0030\n",
            "Epoch 681/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9495 - val_loss: 1.0032\n",
            "Epoch 682/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9502 - val_loss: 1.0017\n",
            "Epoch 683/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9503 - val_loss: 1.0030\n",
            "Epoch 684/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0028\n",
            "Epoch 685/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9507 - val_loss: 1.0015\n",
            "Epoch 686/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9508 - val_loss: 1.0030\n",
            "Epoch 687/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9504 - val_loss: 1.0027\n",
            "Epoch 688/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9491 - val_loss: 1.0026\n",
            "Epoch 689/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9504 - val_loss: 1.0031\n",
            "Epoch 690/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9496 - val_loss: 1.0037\n",
            "Epoch 691/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9497 - val_loss: 1.0027\n",
            "Epoch 692/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9496 - val_loss: 1.0029\n",
            "Epoch 693/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0035\n",
            "Epoch 694/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9504 - val_loss: 1.0035\n",
            "Epoch 695/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9500 - val_loss: 1.0025\n",
            "Epoch 696/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9500 - val_loss: 1.0038\n",
            "Epoch 697/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0028\n",
            "Epoch 698/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9506 - val_loss: 1.0028\n",
            "Epoch 699/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9501 - val_loss: 1.0030\n",
            "Epoch 700/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9504 - val_loss: 1.0030\n",
            "Epoch 701/1000\n",
            "2000/2000 [==============================] - 0s 173us/step - loss: 0.9487 - val_loss: 1.0036\n",
            "Epoch 702/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9498 - val_loss: 1.0039\n",
            "Epoch 703/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9504 - val_loss: 1.0028\n",
            "Epoch 704/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9493 - val_loss: 1.0022\n",
            "Epoch 705/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9502 - val_loss: 1.0026\n",
            "Epoch 706/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0034\n",
            "Epoch 707/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9507 - val_loss: 1.0026\n",
            "Epoch 708/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9496 - val_loss: 1.0019\n",
            "Epoch 709/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0021\n",
            "Epoch 710/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9503 - val_loss: 1.0025\n",
            "Epoch 711/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9506 - val_loss: 1.0018\n",
            "Epoch 712/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9496 - val_loss: 1.0019\n",
            "Epoch 713/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0020\n",
            "Epoch 714/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9493 - val_loss: 1.0028\n",
            "Epoch 715/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9501 - val_loss: 1.0021\n",
            "Epoch 716/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9500 - val_loss: 1.0024\n",
            "Epoch 717/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0020\n",
            "Epoch 718/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9491 - val_loss: 1.0032\n",
            "Epoch 719/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9510 - val_loss: 1.0035\n",
            "Epoch 720/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9504 - val_loss: 1.0015\n",
            "Epoch 721/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9502 - val_loss: 1.0022\n",
            "Epoch 722/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0023\n",
            "Epoch 723/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9504 - val_loss: 1.0024\n",
            "Epoch 724/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9497 - val_loss: 1.0025\n",
            "Epoch 725/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0029\n",
            "Epoch 726/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9499 - val_loss: 1.0031\n",
            "Epoch 727/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9496 - val_loss: 1.0035\n",
            "Epoch 728/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9499 - val_loss: 1.0031\n",
            "Epoch 729/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9495 - val_loss: 1.0036\n",
            "Epoch 730/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9505 - val_loss: 1.0023\n",
            "Epoch 731/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0027\n",
            "Epoch 732/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9506 - val_loss: 1.0025\n",
            "Epoch 733/1000\n",
            "2000/2000 [==============================] - 0s 196us/step - loss: 0.9504 - val_loss: 1.0014\n",
            "Epoch 734/1000\n",
            "2000/2000 [==============================] - 0s 191us/step - loss: 0.9497 - val_loss: 1.0036\n",
            "Epoch 735/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9498 - val_loss: 1.0019\n",
            "Epoch 736/1000\n",
            "2000/2000 [==============================] - 0s 194us/step - loss: 0.9506 - val_loss: 1.0029\n",
            "Epoch 737/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9501 - val_loss: 1.0030\n",
            "Epoch 738/1000\n",
            "2000/2000 [==============================] - 0s 195us/step - loss: 0.9497 - val_loss: 1.0027\n",
            "Epoch 739/1000\n",
            "2000/2000 [==============================] - 0s 200us/step - loss: 0.9504 - val_loss: 1.0026\n",
            "Epoch 740/1000\n",
            "2000/2000 [==============================] - 0s 191us/step - loss: 0.9497 - val_loss: 1.0024\n",
            "Epoch 741/1000\n",
            "2000/2000 [==============================] - 0s 196us/step - loss: 0.9500 - val_loss: 1.0026\n",
            "Epoch 742/1000\n",
            "2000/2000 [==============================] - 0s 192us/step - loss: 0.9503 - val_loss: 1.0020\n",
            "Epoch 743/1000\n",
            "2000/2000 [==============================] - 0s 193us/step - loss: 0.9502 - val_loss: 1.0024\n",
            "Epoch 744/1000\n",
            "2000/2000 [==============================] - 0s 197us/step - loss: 0.9506 - val_loss: 1.0036\n",
            "Epoch 745/1000\n",
            "2000/2000 [==============================] - 0s 194us/step - loss: 0.9501 - val_loss: 1.0035\n",
            "Epoch 746/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9503 - val_loss: 1.0027\n",
            "Epoch 747/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9507 - val_loss: 1.0029\n",
            "Epoch 748/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9506 - val_loss: 1.0022\n",
            "Epoch 749/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9498 - val_loss: 1.0024\n",
            "Epoch 750/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9508 - val_loss: 1.0027\n",
            "Epoch 751/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0019\n",
            "Epoch 752/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9497 - val_loss: 1.0034\n",
            "Epoch 753/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9494 - val_loss: 1.0025\n",
            "Epoch 754/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9504 - val_loss: 1.0028\n",
            "Epoch 755/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9499 - val_loss: 1.0033\n",
            "Epoch 756/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9499 - val_loss: 1.0024\n",
            "Epoch 757/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9501 - val_loss: 1.0027\n",
            "Epoch 758/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9499 - val_loss: 1.0027\n",
            "Epoch 759/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9493 - val_loss: 1.0028\n",
            "Epoch 760/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0025\n",
            "Epoch 761/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9505 - val_loss: 1.0024\n",
            "Epoch 762/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9491 - val_loss: 1.0022\n",
            "Epoch 763/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9498 - val_loss: 1.0027\n",
            "Epoch 764/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9494 - val_loss: 1.0032\n",
            "Epoch 765/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0021\n",
            "Epoch 766/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9508 - val_loss: 1.0033\n",
            "Epoch 767/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9500 - val_loss: 1.0027\n",
            "Epoch 768/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0029\n",
            "Epoch 769/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9500 - val_loss: 1.0034\n",
            "Epoch 770/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9509 - val_loss: 1.0021\n",
            "Epoch 771/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9503 - val_loss: 1.0021\n",
            "Epoch 772/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9508 - val_loss: 1.0016\n",
            "Epoch 773/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9500 - val_loss: 1.0035\n",
            "Epoch 774/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0031\n",
            "Epoch 775/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9498 - val_loss: 1.0023\n",
            "Epoch 776/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9498 - val_loss: 1.0020\n",
            "Epoch 777/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9499 - val_loss: 1.0035\n",
            "Epoch 778/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9500 - val_loss: 1.0022\n",
            "Epoch 779/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9504 - val_loss: 1.0031\n",
            "Epoch 780/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0018\n",
            "Epoch 781/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0017\n",
            "Epoch 782/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0028\n",
            "Epoch 783/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9499 - val_loss: 1.0034\n",
            "Epoch 784/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9495 - val_loss: 1.0027\n",
            "Epoch 785/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9502 - val_loss: 1.0031\n",
            "Epoch 786/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9499 - val_loss: 1.0025\n",
            "Epoch 787/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0031\n",
            "Epoch 788/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9502 - val_loss: 1.0030\n",
            "Epoch 789/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9499 - val_loss: 1.0027\n",
            "Epoch 790/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0024\n",
            "Epoch 791/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9495 - val_loss: 1.0024\n",
            "Epoch 792/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9494 - val_loss: 1.0029\n",
            "Epoch 793/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9507 - val_loss: 1.0040\n",
            "Epoch 794/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9496 - val_loss: 1.0035\n",
            "Epoch 795/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9490 - val_loss: 1.0037\n",
            "Epoch 796/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9504 - val_loss: 1.0026\n",
            "Epoch 797/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9504 - val_loss: 1.0027\n",
            "Epoch 798/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9495 - val_loss: 1.0034\n",
            "Epoch 799/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9504 - val_loss: 1.0025\n",
            "Epoch 800/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9503 - val_loss: 1.0026\n",
            "Epoch 801/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9503 - val_loss: 1.0024\n",
            "Epoch 802/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0020\n",
            "Epoch 803/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9502 - val_loss: 1.0031\n",
            "Epoch 804/1000\n",
            "2000/2000 [==============================] - 0s 174us/step - loss: 0.9505 - val_loss: 1.0029\n",
            "Epoch 805/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0024\n",
            "Epoch 806/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9492 - val_loss: 1.0016\n",
            "Epoch 807/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9498 - val_loss: 1.0027\n",
            "Epoch 808/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9501 - val_loss: 1.0015\n",
            "Epoch 809/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9503 - val_loss: 1.0030\n",
            "Epoch 810/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9494 - val_loss: 1.0030\n",
            "Epoch 811/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9495 - val_loss: 1.0035\n",
            "Epoch 812/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9505 - val_loss: 1.0023\n",
            "Epoch 813/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9498 - val_loss: 1.0027\n",
            "Epoch 814/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9503 - val_loss: 1.0029\n",
            "Epoch 815/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9496 - val_loss: 1.0038\n",
            "Epoch 816/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9498 - val_loss: 1.0031\n",
            "Epoch 817/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9487 - val_loss: 1.0028\n",
            "Epoch 818/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0027\n",
            "Epoch 819/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9503 - val_loss: 1.0031\n",
            "Epoch 820/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9505 - val_loss: 1.0025\n",
            "Epoch 821/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9492 - val_loss: 1.0026\n",
            "Epoch 822/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9493 - val_loss: 1.0025\n",
            "Epoch 823/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9507 - val_loss: 1.0025\n",
            "Epoch 824/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0028\n",
            "Epoch 825/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9499 - val_loss: 1.0030\n",
            "Epoch 826/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9496 - val_loss: 1.0019\n",
            "Epoch 827/1000\n",
            "2000/2000 [==============================] - 0s 188us/step - loss: 0.9511 - val_loss: 1.0040\n",
            "Epoch 828/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9497 - val_loss: 1.0034\n",
            "Epoch 829/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9496 - val_loss: 1.0037\n",
            "Epoch 830/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9499 - val_loss: 1.0031\n",
            "Epoch 831/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9507 - val_loss: 1.0025\n",
            "Epoch 832/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9495 - val_loss: 1.0021\n",
            "Epoch 833/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9501 - val_loss: 1.0032\n",
            "Epoch 834/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9501 - val_loss: 1.0029\n",
            "Epoch 835/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9494 - val_loss: 1.0020\n",
            "Epoch 836/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9490 - val_loss: 1.0029\n",
            "Epoch 837/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9509 - val_loss: 1.0022\n",
            "Epoch 838/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9494 - val_loss: 1.0024\n",
            "Epoch 839/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0032\n",
            "Epoch 840/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9507 - val_loss: 1.0019\n",
            "Epoch 841/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9496 - val_loss: 1.0033\n",
            "Epoch 842/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9494 - val_loss: 1.0040\n",
            "Epoch 843/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9508 - val_loss: 1.0020\n",
            "Epoch 844/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9497 - val_loss: 1.0021\n",
            "Epoch 845/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9500 - val_loss: 1.0027\n",
            "Epoch 846/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9502 - val_loss: 1.0038\n",
            "Epoch 847/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9506 - val_loss: 1.0028\n",
            "Epoch 848/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0038\n",
            "Epoch 849/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9505 - val_loss: 1.0037\n",
            "Epoch 850/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9498 - val_loss: 1.0025\n",
            "Epoch 851/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9498 - val_loss: 1.0026\n",
            "Epoch 852/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9501 - val_loss: 1.0028\n",
            "Epoch 853/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9500 - val_loss: 1.0022\n",
            "Epoch 854/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9493 - val_loss: 1.0040\n",
            "Epoch 855/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0034\n",
            "Epoch 856/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9506 - val_loss: 1.0023\n",
            "Epoch 857/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9504 - val_loss: 1.0019\n",
            "Epoch 858/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9506 - val_loss: 1.0028\n",
            "Epoch 859/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9504 - val_loss: 1.0018\n",
            "Epoch 860/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0024\n",
            "Epoch 861/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9504 - val_loss: 1.0023\n",
            "Epoch 862/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9502 - val_loss: 1.0020\n",
            "Epoch 863/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9494 - val_loss: 1.0022\n",
            "Epoch 864/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0028\n",
            "Epoch 865/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9496 - val_loss: 1.0028\n",
            "Epoch 866/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9499 - val_loss: 1.0032\n",
            "Epoch 867/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9497 - val_loss: 1.0029\n",
            "Epoch 868/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0022\n",
            "Epoch 869/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9497 - val_loss: 1.0024\n",
            "Epoch 870/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0027\n",
            "Epoch 871/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9502 - val_loss: 1.0033\n",
            "Epoch 872/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9499 - val_loss: 1.0031\n",
            "Epoch 873/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9504 - val_loss: 1.0025\n",
            "Epoch 874/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9500 - val_loss: 1.0029\n",
            "Epoch 875/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9509 - val_loss: 1.0025\n",
            "Epoch 876/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9500 - val_loss: 1.0032\n",
            "Epoch 877/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9492 - val_loss: 1.0037\n",
            "Epoch 878/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9497 - val_loss: 1.0038\n",
            "Epoch 879/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9496 - val_loss: 1.0027\n",
            "Epoch 880/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9499 - val_loss: 1.0027\n",
            "Epoch 881/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9497 - val_loss: 1.0024\n",
            "Epoch 882/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9490 - val_loss: 1.0028\n",
            "Epoch 883/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9493 - val_loss: 1.0024\n",
            "Epoch 884/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9493 - val_loss: 1.0030\n",
            "Epoch 885/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9504 - val_loss: 1.0033\n",
            "Epoch 886/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9503 - val_loss: 1.0032\n",
            "Epoch 887/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9499 - val_loss: 1.0021\n",
            "Epoch 888/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9498 - val_loss: 1.0035\n",
            "Epoch 889/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9494 - val_loss: 1.0029\n",
            "Epoch 890/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9503 - val_loss: 1.0027\n",
            "Epoch 891/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9494 - val_loss: 1.0025\n",
            "Epoch 892/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0027\n",
            "Epoch 893/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9494 - val_loss: 1.0027\n",
            "Epoch 894/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9501 - val_loss: 1.0023\n",
            "Epoch 895/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9506 - val_loss: 1.0020\n",
            "Epoch 896/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9494 - val_loss: 1.0034\n",
            "Epoch 897/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9499 - val_loss: 1.0029\n",
            "Epoch 898/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0021\n",
            "Epoch 899/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9507 - val_loss: 1.0022\n",
            "Epoch 900/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9489 - val_loss: 1.0036\n",
            "Epoch 901/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9502 - val_loss: 1.0027\n",
            "Epoch 902/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9502 - val_loss: 1.0031\n",
            "Epoch 903/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9501 - val_loss: 1.0027\n",
            "Epoch 904/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0027\n",
            "Epoch 905/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9511 - val_loss: 1.0020\n",
            "Epoch 906/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9502 - val_loss: 1.0034\n",
            "Epoch 907/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9500 - val_loss: 1.0036\n",
            "Epoch 908/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9499 - val_loss: 1.0035\n",
            "Epoch 909/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9498 - val_loss: 1.0030\n",
            "Epoch 910/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9494 - val_loss: 1.0017\n",
            "Epoch 911/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9497 - val_loss: 1.0023\n",
            "Epoch 912/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9506 - val_loss: 1.0030\n",
            "Epoch 913/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9500 - val_loss: 1.0024\n",
            "Epoch 914/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9505 - val_loss: 1.0031\n",
            "Epoch 915/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9497 - val_loss: 1.0023\n",
            "Epoch 916/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9500 - val_loss: 1.0029\n",
            "Epoch 917/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9492 - val_loss: 1.0037\n",
            "Epoch 918/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9512 - val_loss: 1.0035\n",
            "Epoch 919/1000\n",
            "2000/2000 [==============================] - 0s 190us/step - loss: 0.9508 - val_loss: 1.0033\n",
            "Epoch 920/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9503 - val_loss: 1.0038\n",
            "Epoch 921/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9508 - val_loss: 1.0025\n",
            "Epoch 922/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9498 - val_loss: 1.0029\n",
            "Epoch 923/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9497 - val_loss: 1.0030\n",
            "Epoch 924/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9497 - val_loss: 1.0033\n",
            "Epoch 925/1000\n",
            "2000/2000 [==============================] - 0s 176us/step - loss: 0.9502 - val_loss: 1.0030\n",
            "Epoch 926/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9502 - val_loss: 1.0033\n",
            "Epoch 927/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9496 - val_loss: 1.0026\n",
            "Epoch 928/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9509 - val_loss: 1.0016\n",
            "Epoch 929/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0026\n",
            "Epoch 930/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0029\n",
            "Epoch 931/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9503 - val_loss: 1.0025\n",
            "Epoch 932/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9496 - val_loss: 1.0042\n",
            "Epoch 933/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9495 - val_loss: 1.0026\n",
            "Epoch 934/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9502 - val_loss: 1.0020\n",
            "Epoch 935/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9507 - val_loss: 1.0019\n",
            "Epoch 936/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0034\n",
            "Epoch 937/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9509 - val_loss: 1.0028\n",
            "Epoch 938/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0034\n",
            "Epoch 939/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9497 - val_loss: 1.0040\n",
            "Epoch 940/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9505 - val_loss: 1.0030\n",
            "Epoch 941/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9493 - val_loss: 1.0033\n",
            "Epoch 942/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0032\n",
            "Epoch 943/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9502 - val_loss: 1.0029\n",
            "Epoch 944/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9497 - val_loss: 1.0035\n",
            "Epoch 945/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9501 - val_loss: 1.0030\n",
            "Epoch 946/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9492 - val_loss: 1.0034\n",
            "Epoch 947/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9501 - val_loss: 1.0023\n",
            "Epoch 948/1000\n",
            "2000/2000 [==============================] - 0s 177us/step - loss: 0.9500 - val_loss: 1.0032\n",
            "Epoch 949/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0023\n",
            "Epoch 950/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9495 - val_loss: 1.0031\n",
            "Epoch 951/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0030\n",
            "Epoch 952/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9494 - val_loss: 1.0027\n",
            "Epoch 953/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9508 - val_loss: 1.0035\n",
            "Epoch 954/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9499 - val_loss: 1.0025\n",
            "Epoch 955/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9503 - val_loss: 1.0028\n",
            "Epoch 956/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9505 - val_loss: 1.0026\n",
            "Epoch 957/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9500 - val_loss: 1.0022\n",
            "Epoch 958/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9503 - val_loss: 1.0013\n",
            "Epoch 959/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9502 - val_loss: 1.0034\n",
            "Epoch 960/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9499 - val_loss: 1.0031\n",
            "Epoch 961/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9497 - val_loss: 1.0029\n",
            "Epoch 962/1000\n",
            "2000/2000 [==============================] - 0s 189us/step - loss: 0.9501 - val_loss: 1.0026\n",
            "Epoch 963/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9505 - val_loss: 1.0035\n",
            "Epoch 964/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9504 - val_loss: 1.0021\n",
            "Epoch 965/1000\n",
            "2000/2000 [==============================] - 0s 180us/step - loss: 0.9491 - val_loss: 1.0031\n",
            "Epoch 966/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9504 - val_loss: 1.0031\n",
            "Epoch 967/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9500 - val_loss: 1.0027\n",
            "Epoch 968/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9501 - val_loss: 1.0026\n",
            "Epoch 969/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9504 - val_loss: 1.0039\n",
            "Epoch 970/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0022\n",
            "Epoch 971/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9506 - val_loss: 1.0024\n",
            "Epoch 972/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9496 - val_loss: 1.0023\n",
            "Epoch 973/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9498 - val_loss: 1.0030\n",
            "Epoch 974/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9497 - val_loss: 1.0027\n",
            "Epoch 975/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9499 - val_loss: 1.0028\n",
            "Epoch 976/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9503 - val_loss: 1.0025\n",
            "Epoch 977/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9501 - val_loss: 1.0031\n",
            "Epoch 978/1000\n",
            "2000/2000 [==============================] - 0s 179us/step - loss: 0.9503 - val_loss: 1.0035\n",
            "Epoch 979/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9501 - val_loss: 1.0024\n",
            "Epoch 980/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9505 - val_loss: 1.0028\n",
            "Epoch 981/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9503 - val_loss: 1.0024\n",
            "Epoch 982/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9496 - val_loss: 1.0027\n",
            "Epoch 983/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9501 - val_loss: 1.0028\n",
            "Epoch 984/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9496 - val_loss: 1.0026\n",
            "Epoch 985/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9499 - val_loss: 1.0037\n",
            "Epoch 986/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9499 - val_loss: 1.0021\n",
            "Epoch 987/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9499 - val_loss: 1.0017\n",
            "Epoch 988/1000\n",
            "2000/2000 [==============================] - 0s 185us/step - loss: 0.9497 - val_loss: 1.0030\n",
            "Epoch 989/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9498 - val_loss: 1.0032\n",
            "Epoch 990/1000\n",
            "2000/2000 [==============================] - 0s 178us/step - loss: 0.9496 - val_loss: 1.0031\n",
            "Epoch 991/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9494 - val_loss: 1.0030\n",
            "Epoch 992/1000\n",
            "2000/2000 [==============================] - 0s 186us/step - loss: 0.9495 - val_loss: 1.0035\n",
            "Epoch 993/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9504 - val_loss: 1.0022\n",
            "Epoch 994/1000\n",
            "2000/2000 [==============================] - 0s 187us/step - loss: 0.9496 - val_loss: 1.0027\n",
            "Epoch 995/1000\n",
            "2000/2000 [==============================] - 0s 183us/step - loss: 0.9505 - val_loss: 1.0028\n",
            "Epoch 996/1000\n",
            "2000/2000 [==============================] - 0s 182us/step - loss: 0.9492 - val_loss: 1.0031\n",
            "Epoch 997/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9500 - val_loss: 1.0021\n",
            "Epoch 998/1000\n",
            "2000/2000 [==============================] - 0s 181us/step - loss: 0.9499 - val_loss: 1.0027\n",
            "Epoch 999/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9500 - val_loss: 1.0026\n",
            "Epoch 1000/1000\n",
            "2000/2000 [==============================] - 0s 184us/step - loss: 0.9496 - val_loss: 1.0033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f58e8c048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8OxCmH0q-3W",
        "colab_type": "code",
        "outputId": "22855f03-7239-42c2-9841-7ab68181c543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_val = model.predict(x = val_data, verbose = 0)\n",
        "pred_test = model.predict(x = test_data, verbose = 0)\n",
        "\n",
        "log_loss(test_label, pred_test), log_loss(val_label, pred_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9703576371073723, 1.0027130029537605)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNR-i7XEveec",
        "colab_type": "code",
        "outputId": "471a5d29-7cdf-4898-9072-1c9a6693685e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "pred = model.predict(test_data)\n",
        "pred_test = [np.argmax(row) for row in pred]\n",
        "y = [np.argmax(row) for row in test_label]\n",
        "\n",
        "precision = precision_score(pred_test,y,average='weighted')\n",
        "recall = recall_score(pred_test,y,average='weighted')\n",
        "f1 = f1_score(pred_test,y,average='weighted')\n",
        "accuracy = accuracy_score(pred_test, y)\n",
        "\n",
        "print('precision:',precision,'\\nrecall:',recall,'\\nf1:',f1,'\\nacc:',accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: 1.0 \n",
            "recall: 0.4275 \n",
            "f1: 0.5989492119089317 \n",
            "acc: 0.4275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHB1BXm9SHy2",
        "colab_type": "text"
      },
      "source": [
        "### Crossval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfAJa4RK6VqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = emb[0][:2454]\n",
        "test_data = emb[0][2454:]\n",
        "train_label = emb[1][:2454]\n",
        "test_label = emb[1][2454:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfRXM__t7HeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_mlp_model(input_shape):\n",
        "\tX_input = layers.Input(input_shape)\n",
        "\n",
        "\t# First dense layer\n",
        "\tX = layers.Dense(300, name = 'dense0')(X_input)\n",
        "\tX = layers.BatchNormalization(name = 'bn0')(X)\n",
        "\tX = layers.Activation('relu')(X)\n",
        "\tX = layers.Dropout(0.5, seed = 7)(X)\n",
        "\n",
        "\t# Output layer\n",
        "\tX = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(0.1))(X)\n",
        "\tX = layers.Activation('softmax')(X)\n",
        "\n",
        "\t# Create model\n",
        "\tmodel = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrQ0oUeV1y_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folds = KFold(n_splits=5, shuffle=True, random_state=3)\n",
        "scores = []\n",
        "prediction = np.zeros((len(test_data),3))\n",
        "for fold_n, (train_index, valid_index) in enumerate(folds.split(train_data)):\n",
        "\tprint('Fold', fold_n, 'started at', time.ctime())\n",
        "\tX_tr, X_val = train_data[train_index], train_data[valid_index]\n",
        "\tY_tr, Y_val = train_label[train_index], train_label[valid_index]\n",
        "\n",
        "\t# Define the model, re-initializing for each fold\n",
        "\tclassif_model = build_mlp_model([train_data.shape[1]])\n",
        "\tclassif_model.compile(optimizer = optimizers.Adam(lr = 0.001), loss = \"categorical_crossentropy\")\n",
        "\n",
        "\tclassif_model.fit(x = X_tr, y = Y_tr, epochs = 1000, batch_size = 32, validation_data = (X_val, Y_val), verbose = 0)\n",
        "\n",
        "\t# make predictions on validation and test data\n",
        "\tpred_valid = classif_model.predict(x = X_val, verbose = 0)\n",
        "\tpred = classif_model.predict(x = test_data, verbose = 0)\n",
        "\n",
        "\t# oof[valid_index] = pred_valid.reshape(-1,)\n",
        "\tscores.append(log_loss(Y_val, pred_valid))\n",
        "\tprediction += pred\n",
        "prediction /= 5\n",
        "\n",
        "# Print CV scores, as well as score on the test data\n",
        "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
        "print(scores)\n",
        "print(\"Test score:\", log_loss(test_label,prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W42CyD65x_m",
        "colab_type": "code",
        "outputId": "6ca2e97b-1959-4944-db03-78acd728deec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "prediction /= 5\n",
        "\n",
        "# Print CV scores, as well as score on the test data\n",
        "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
        "print(scores)\n",
        "print(\"Test score:\", log_loss(test_label,prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV mean score: 0.9597, std: 0.0182.\n",
            "[0.9594847553859175, 0.9349472906341864, 0.9501213746983748, 0.9902845710940856, 0.9634564618675077]\n",
            "Test score: 0.9698580836856097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYYJ_2ijSkLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = prediction.copy()\n",
        "for i,row in enumerate(pred):\n",
        "  max_ = np.argmax(row)\n",
        "  print(row)\n",
        "  pred[i] = [0,0,0]\n",
        "  pred[i,max_] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUOObUONRtvu",
        "colab_type": "code",
        "outputId": "c09ba60d-6d48-4eec-ac7f-1c7254107849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "precision = precision_score(test_label,pred,average='weighted')\n",
        "recall = recall_score(test_label,pred,average='weighted')\n",
        "f1 = f1_score(test_label,pred,average='weighted')\n",
        "accuracy = accuracy_score(test_label,pred)\n",
        "\n",
        "print('precision:',precision,'\\nrecall:',recall,'\\nf1:',f1,'\\nacc:',accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: 0.18275624999999998 \n",
            "recall: 0.4275 \n",
            "f1: 0.2560507880910683 \n",
            "acc: 0.4275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVG05CcsSTtA",
        "colab_type": "code",
        "outputId": "7629f9f5-0fc9-4092-8ed2-2dd8f0f14b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_data[0][:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.738879 , -0.2271645,  0.109304 , -0.527125 , -0.101143 ,\n",
              "       -0.3374955,  1.4430055, -0.1159995, -0.192594 ,  0.099925 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LwLBEfsTnka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = emb[0][:2454]\n",
        "test_data = emb[0][2454:]\n",
        "train_label = emb[1][:2454]\n",
        "test_label = emb[1][2454:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqITA1yWUFCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = [row for row in range(len(train_data)) if np.sum(np.isnan(train_data[row]))]\n",
        "train_data = np.delete(train_data, idx, 0)\n",
        "train_label = np.delete(train_label, idx, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-K1KkB0U32o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = [row for row in range(len(test_data)) if np.sum(np.isnan(test_data[row]))]\n",
        "test_data = np.delete(test_data, idx, 0)\n",
        "test_label = np.delete(test_label, idx, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4nWRcMXTr7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = [np.argmax(row) for row in train_label]\n",
        "y_test = [np.argmax(row) for row in test_label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqm_l9OacXwQ",
        "colab_type": "code",
        "outputId": "7a6de7f7-6903-4ea0-9d41-3d75af2c662f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(train_data, y_train)\n",
        "pred = clf.predict(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKwyeU5qTJwa",
        "colab_type": "code",
        "outputId": "cde974e5-dab7-4674-ceb6-8c165bd0dca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "precision = precision_score(y_test,pred,average='weighted')\n",
        "recall = recall_score(y_test,pred,average='weighted')\n",
        "f1 = f1_score(y_test,pred,average='weighted')\n",
        "accuracy = accuracy_score(y_test,pred)\n",
        "\n",
        "print('precision:',precision,'\\nrecall:',recall,'\\nf1:',f1,'\\nacc:',accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: 0.7477162193510789 \n",
            "recall: 0.753376688344172 \n",
            "f1: 0.7468932304493048 \n",
            "acc: 0.753376688344172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyLm_tzQVC8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}